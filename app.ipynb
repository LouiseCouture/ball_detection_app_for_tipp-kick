{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7907527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from ipynb.fs.full.fonctions import boundingBoxes,display,drawContour,checkBoxes,substraction\n",
    "from ipynb.fs.full.wavelet import middle,Wavelet\n",
    "from ipynb.fs.full.ransac import doRansac,plotLines\n",
    "from ipynb.fs.full.white_select import selectWhite,selectWhiteHSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88bc3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lc100/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement \"gitpython>=3.1.30\" not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  AutoUpdate skipped (offline)\n",
      "YOLOv5  2023-6-14 Python-3.11.3 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "def detect_ball(model, frame: np.ndarray):\n",
    "    WIDTH = 360\n",
    "    HEIGHT = 360\n",
    "    resized_frame = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "    detection = model(resized_frame)\n",
    "    bounding_box = detection.xyxy[0].numpy()\n",
    "\n",
    "    for box in bounding_box:\n",
    "        if box[5] == 0:\n",
    "            x = box[0] + (box[2] - box[0]) / 2\n",
    "            y = box[1] + (box[3] - box[1]) / 2\n",
    "            return (int(x), int(y)), (box[0], box[1], box[2], box[3])\n",
    "            # array([     124.15,         179,      130.78,      185.98,      0.8651,           0], dtype=float32)\n",
    "            # box[0]:   Left\n",
    "            # box[1]:   Top\n",
    "            # box[2]:   Right\n",
    "            # box[3]:   Bottom\n",
    "            # box[4]:   Probability\n",
    "            # box[5]:   Klasse 0 = Ball\n",
    "\n",
    "    return False, False\n",
    "\n",
    "ball_model = torch.hub.load('ultralytics/yolov5', 'custom', path='03_Ball_Detection/models/ball_weights_V2.pt', force_reload=False)\n",
    "\n",
    "def detect_ball_static(image,model=ball_model):\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # start object detection\n",
    "    # ball detection returns center coordinates from the ball\n",
    "    ball_center, ball_bb = detect_ball(model, image)\n",
    "    if ball_center:  \n",
    "        ball_center = [ball_center[0] * (width / 360), ball_center[1] * (height / 360)] \n",
    "        ball_bb=[int(ball_bb[0]* (width / 360)),\n",
    "                int(ball_bb[1]* (height / 360)),\n",
    "                int(ball_bb[2]* (width / 360)),\n",
    "                int(ball_bb[3]* (height / 360))]  \n",
    "        \n",
    "        return [ball_bb[0],ball_bb[1],ball_bb[2]-ball_bb[0]+10,ball_bb[3]-ball_bb[1]+10]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78182fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 frames for the substraction\n",
    "frame1=None #newest frame\n",
    "frame0=None #previous frame\n",
    "\n",
    "# 1st frame will be use to detect the white borders of the field\n",
    "first_frame=None\n",
    "white_border=None\n",
    "\n",
    "# index is the time of the frame\n",
    "idx=0\n",
    "# time jump between 2 frame\n",
    "jump=100\n",
    "\n",
    "#3D points (x,y,t,type)\n",
    "position=[[0,0,0,0]]\n",
    "#number of pts used for RANSAC\n",
    "size_slice=10\n",
    "#pts after ransac\n",
    "corrected=np.array([[0,0,0,0]])\n",
    "#pts after correction of the trajectory\n",
    "corrected2=np.array([[0,0,0,0]])\n",
    "# index of the last pts that had is trajectory corrected in corrected[]\n",
    "lastcorrected=0 \n",
    "firstLoop=True\n",
    "lastLen=0\n",
    "\n",
    "# all the lines found by RANSAC\n",
    "all_lines=[ [ [[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]] ] ]\n",
    "#all_lines=[[[0,0,0],[0,0,0]]]\n",
    "\n",
    "model=None\n",
    "lastPosition=None\n",
    "\n",
    "static=False\n",
    "static_box=None\n",
    "\n",
    "#templates used for the wavelet function\n",
    "template = cv2.imread('template/templateRGB_Black.png')\n",
    "template_HSV_W= cv2.cvtColor(template,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "templateB = cv2.imread('template/templateB.png')\n",
    "template_HSV_B = Wavelet(templateB)\n",
    "\n",
    "#video\n",
    "cap = cv2.VideoCapture('video_record/1310.mp4')\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56233ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6666666666666665\n",
      "3.5833333333333335\n",
      "2.1666666666666665\n",
      "3.25\n",
      "3.1666666666666665\n",
      "3.9166666666666665\n",
      "3.25\n",
      "3.4166666666666665\n",
      "3.75\n",
      "3.3333333333333335\n",
      "4.083333333333333\n",
      "3.9166666666666665\n",
      "2.6666666666666665\n",
      "3.8333333333333335\n",
      "3.5833333333333335\n",
      "4.166666666666667\n",
      "4.75\n",
      "4.75\n",
      "4.916666666666667\n",
      "4.333333333333333\n",
      "4.5\n",
      "3.5\n",
      "3.5833333333333335\n",
      "2.75\n",
      "1.8333333333333333\n",
      "2.75\n",
      "4.25\n",
      "3.8333333333333335\n",
      "3.5833333333333335\n",
      "4.083333333333333\n",
      "4.333333333333333\n",
      "2.8333333333333335\n",
      "2.4166666666666665\n",
      "3.1666666666666665\n",
      "3.0\n",
      "2.0833333333333335\n",
      "2.5\n",
      "2.5833333333333335\n",
      "3.25\n",
      "2.75\n",
      "2.8333333333333335\n",
      "2.5\n",
      "4.416666666666667\n",
      "2.25\n",
      "2.8333333333333335\n",
      "4.166666666666667\n",
      "2.75\n",
      "2.5\n",
      "2.5833333333333335\n",
      "2.75\n",
      "3.1666666666666665\n",
      "2.9166666666666665\n",
      "2.5833333333333335\n",
      "3.0\n",
      "3.9166666666666665\n",
      "4.916666666666667\n",
      "4.583333333333333\n",
      "3.5833333333333335\n",
      "4.75\n",
      "4.666666666666667\n",
      "4.666666666666667\n",
      "3.5833333333333335\n",
      "3.0\n",
      "2.75\n",
      "3.0833333333333335\n",
      "3.0833333333333335\n",
      "3.75\n",
      "3.25\n",
      "2.0833333333333335\n",
      "2.4166666666666665\n",
      "2.6666666666666665\n",
      "3.0833333333333335\n",
      "2.4166666666666665\n",
      "3.4166666666666665\n",
      "3.9166666666666665\n",
      "2.0833333333333335\n",
      "4.416666666666667\n",
      "2.6666666666666665\n",
      "2.5833333333333335\n",
      "2.9166666666666665\n",
      "1.9166666666666667\n",
      "2.4166666666666665\n",
      "2.75\n",
      "1.9166666666666667\n",
      "2.5833333333333335\n",
      "3.5833333333333335\n",
      "2.9166666666666665\n",
      "4.333333333333333\n",
      "2.25\n",
      "2.1666666666666665\n",
      "2.0833333333333335\n",
      "2.5833333333333335\n"
     ]
    }
   ],
   "source": [
    "#Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame_og= cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        model=None\n",
    "        \n",
    "        frame_og_crop=frame_og[:,:1700,:].copy()\n",
    "        last_frame = frame_og_crop.copy()\n",
    "\n",
    "        #detect the white border so to not confuse them with the ball\n",
    "        if first_frame is None:\n",
    "            frame0=frame_og_crop.copy()\n",
    "            frame1=frame_og_crop.copy()\n",
    "            \n",
    "            first_frame = frame_og_crop.copy()\n",
    "            white_border=selectWhite(first_frame,dilate=9,erode=3)\n",
    "\n",
    "            white_border=cv2.bitwise_not(white_border)\n",
    "\n",
    "            display(white_border,name='not border')\n",
    "\n",
    "    \n",
    "        frame0 = frame1.copy()\n",
    "        frame1 = frame_og_crop.copy()\n",
    "\n",
    "        #white select\n",
    "        #maskW1=selectWhite(frame1,erode=3,dilate=8,B_limit=70)\n",
    "        #maskW0=selectWhite(frame0,erode=3,dilate=8,B_limit=70)\n",
    "\n",
    "        #maskW=cv2.bitwise_or(maskW1, maskW0, mask = None)\n",
    "        #maskW=cv2.bitwise_and(maskW,white_border, mask=None)\n",
    "        #maskW = cv2.erode(maskW, None, iterations=4)\n",
    "        #maskWA = cv2.dilate(maskW, None, iterations=15)\n",
    "        \n",
    "        #frame_white=cv2.bitwise_and(frame_og_crop.copy(),frame_og_crop.copy(), mask=maskW)\n",
    "        #display(frame_white,name='white mask RGB')\n",
    "        \n",
    "        \n",
    "        #substraction\n",
    "        frameHLS0=cv2.cvtColor(frame0,cv2.COLOR_BGR2YUV)\n",
    "        frameHLS1=cv2.cvtColor(frame1,cv2.COLOR_BGR2YUV)\n",
    "        frame_substraction=substraction(frameHLS0[:,:,0],frameHLS1[:,:,0],blur_type=0,threshold_type=0,threshold=30,blur=21,show=True,erode=4,dilate=5)\n",
    "        \n",
    "        #white select\n",
    "        maskedW_sub=cv2.bitwise_and(frame1, frame1, mask=frame_substraction)\n",
    "        maskedW_sub=selectWhite(maskedW_sub,erode=3,dilate=8,B_limit=0,W_limit=100,H_limit=40)\n",
    "\n",
    "        maskW = cv2.bitwise_and(maskedW_sub , white_border, mask=None)\n",
    "        maskW = cv2.erode(maskW, None, iterations=4)\n",
    "        maskW = cv2.dilate(maskW, None, iterations=10)\n",
    "        \n",
    "        frame_white=cv2.bitwise_and(frame_og_crop.copy(),frame_og_crop.copy(), mask=maskW)\n",
    "        display(frame_white,name='white mask sub RGB')\n",
    "        \n",
    "        #whiteANDsub=cv2.bitwise_and(frame_substraction,maskW, mask=None)\n",
    "        #display(whiteANDsub,name='both sub and white')\n",
    "        \n",
    "        #draw contour on frame\n",
    "        dil,contours=drawContour(maskW,kernelsize=15)\n",
    "        display(dil,name='dilatation')\n",
    "        #hide non-contoured area on RGB\n",
    "        masked1=cv2.bitwise_and(frame1, frame1, mask=dil)\n",
    "        #put boxes around objectsq\n",
    "        _,boundRect=boundingBoxes(dil,contours,show=True)\n",
    "        #detect the ball in one of the boxes (if any)\n",
    "        _, box = checkBoxes(frame_og_crop.copy(),boundRect,coeff=10,wave_temp=template_HSV_W,wave_temp2=template_HSV_B)\n",
    "        \n",
    "        #static detection\n",
    "        if (box is None and not static) : #did not detected moving ball, static mode on\n",
    "            static=True    \n",
    "            box=detect_ball_static(frame_og_crop.copy(),ball_model)     \n",
    "            static_box= box\n",
    "        elif box is None and static_box is not None: # still not moving, use the last detection\n",
    "            box=static_box\n",
    "        else: # moving, static mode off\n",
    "            static_box=None\n",
    "            static=False   \n",
    "        \n",
    "        # add the 3D to the points\n",
    "        if box is not None:\n",
    "            pts=middle(box)\n",
    "            \n",
    "            # if same position, don't take it.\n",
    "            if not np.array_equal(pts, position[-1][:2]) and ((pts[0]-position[-1][0])**2+(pts[1]-position[-1][1])**2)>100:\n",
    "                if static:\n",
    "                    pts=[pts[0],pts[1],idx,1]\n",
    "                else:\n",
    "                    pts=[pts[0],pts[1],idx,0]\n",
    "                position=np.append(position,[pts],axis=0) \n",
    "                idx+=jump\n",
    "\n",
    "            cv2.rectangle(frame_og_crop, (int(box[0]), int(box[1])),(int(box[0]+box[2]), int(box[1]+box[3])), (255,255,255), 2) \n",
    "\n",
    "\n",
    "        #Ransac\n",
    "        Len=len(position)\n",
    "        if Len%size_slice==0 and lastLen!=Len:\n",
    "            if firstLoop:\n",
    "                position=position[1:]\n",
    "\n",
    "            sliced=position[(Len-size_slice):]\n",
    "\n",
    "            #if model is not None:\n",
    "            #    sliced=np.concatenate((corrected[len(corrected)-2:],sliced))\n",
    "\n",
    "            model,droite=doRansac(sliced,frame_og_crop,distMin=15,D3=True)\n",
    "            lastLen=Len\n",
    "\n",
    "            if model is not None:\n",
    "                corrected=np.concatenate((corrected,model))\n",
    "                all_lines=np.append(all_lines,[droite],axis=0)\n",
    "\n",
    "                if firstLoop:\n",
    "                    position=position[1:]\n",
    "                    corrected=corrected[1:]\n",
    "        \n",
    "        # correct trajectory: if i is shorter to go to the 5th pts rather than the next then we jump\n",
    "        k=lastcorrected\n",
    "        lenCorrect=len(corrected)\n",
    "        \n",
    "        while k < (lenCorrect-5) :\n",
    "            diff1=cv2.absdiff(corrected[k],corrected[k+1])\n",
    "            diff2=cv2.absdiff(corrected[k],corrected[k+5])\n",
    "            corrected2=np.append(corrected2,[corrected[k]],axis=0)\n",
    "\n",
    "            if diff1[0]+diff1[1]*1 < (diff2[0]+diff2[1]):\n",
    "                k+=1\n",
    "            else:\n",
    "                k+=4\n",
    "        if firstLoop and model is not None:\n",
    "            corrected2=corrected2[1:]\n",
    "            firstLoop=False\n",
    "        if lenCorrect>5:\n",
    "            lastcorrected = lenCorrect-5\n",
    "\n",
    "\n",
    "        #plot\n",
    "        for index, item in enumerate(position): \n",
    "            if not item[3]: #green is moving\n",
    "                cv2.circle(frame_og_crop, item[:2], 2, [20, 255, 20], 5)\n",
    "            else: # blue is static\n",
    "                cv2.circle(frame_og_crop, item[:2], 2, [255, 200, 50], 5)\n",
    "\n",
    "        plotLines(corrected,lines2=corrected2,frame=frame_og_crop)\n",
    "\n",
    "        display(frame_og_crop,name='detection')\n",
    "    \n",
    "        #press key to go the next frame or Q to exit or S to save the frame\n",
    "        key = cv2.waitKey(0)\n",
    "        \n",
    "        if key == ord('q') or key == ord('Q'):\n",
    "            break\n",
    "        elif key == ord('s') or key == ord('S'):\n",
    "            box_cp = frame_og[box[1]:(box[1]+box[3]),box[0]:(box[0]+box[2]),:]\n",
    "            cv2.imwrite(\"object1_{}.png\".format(idx), box_cp)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "    \n",
    "\n",
    "all_lines=all_lines[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c90e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.grid()\n",
    "\n",
    "ax.plot3D(corrected[:,1],corrected[:,0],corrected[:,2],'r')\n",
    "ax.scatter(position[:,1],position[:,0],position[:,2],marker=\"+\")\n",
    "\n",
    "for item in all_lines:\n",
    "     start_pts=item[0][0]\n",
    "     middle_pts=item[0][1]\n",
    "     end_pts=item[1][1]\n",
    "     \n",
    "     ax.scatter(middle_pts[1],middle_pts[0],middle_pts[2],color='k')\n",
    "     ax.scatter(end_pts[1],end_pts[0],end_pts[2],color='g')\n",
    "     ax.scatter(start_pts[1],start_pts[0],start_pts[2],color='g')\n",
    "     \n",
    "     ax.plot([start_pts[1], middle_pts[1]], [start_pts[0], middle_pts[0]], zs=[start_pts[2], middle_pts[2]],color='k')\n",
    "     ax.plot([end_pts[1], middle_pts[1]], [end_pts[0], middle_pts[0]], zs=[end_pts[2], middle_pts[2]],color='k')\n",
    "\n",
    "\n",
    "ax.set_ylabel('x', labelpad=20)\n",
    "ax.set_ylim(0, 1500)\n",
    "ax.set_xlabel('y', labelpad=20)\n",
    "ax.set_xlim(0, 1500)\n",
    "ax.set_zlabel('t', labelpad=20)\n",
    "#ax.set_zlim(0, 100)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.plot(corrected[:,2],corrected[:,1],'g')\n",
    "plt.plot(corrected[:,2],corrected[:,0],'r')\n",
    "plt.plot(corrected[:,2],corrected[:,1],'g+')\n",
    "plt.plot(corrected[:,2],corrected[:,0],'r+')\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.plot(corrected2[:,2],corrected2[:,1],'k')\n",
    "plt.plot(corrected2[:,2],corrected2[:,0],'b')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plotLines(corrected,corrected2,frame_og_crop)\n",
    "\n",
    "key = cv2.waitKey(0)\n",
    "#When everything done, release the video capture object\n",
    "cap.release()  \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915050e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6ff9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#classifier = keras_cv.models.ImageClassifier.from_preset(\\n#    \"efficientnetv2_b0_imagenet_classifier\"\\n#)\\n\\nimage = keras.utils.load_img(\"Imageterrain.jpg\")\\nimage = np.array(image)\\nkeras_cv.visualization.plot_image_gallery(\\n    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\\n)\\npredictions = classifier.predict(np.expand_dims(image, axis=0))\\ntop_classes = predictions[0].argsort(axis=-1)\\nclasses = keras.utils.get_file(\\n    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\\n)\\nwith open(classes, \"rb\") as f:\\n    classes = json.load(f)\\ntop_two = [classes[str(i)] for i in top_classes[-2:]]\\nprint(\"Top two classes are:\", top_two)\\nBATCH_SIZE = 32\\nIMAGE_SIZE = (224, 224)\\nAUTOTUNE = tf.data.AUTOTUNE\\ntfds.disable_progress_bar()\\n\\ndata, dataset_info = tfds.load(\"cats_vs_dogs\", with_info=True, as_supervised=True)\\ntrain_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\\ntrain_dataset = data[\"train\"]\\n\\nnum_classes = dataset_info.features[\"label\"].num_classes\\n\\nresizing = keras_cv.layers.Resizing(\\n    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\\n)\\n\\n\\ndef preprocess_inputs(image, label):\\n    image = tf.cast(image, tf.float32)\\n    # Staticly resize images as we only iterate the dataset once.\\n    return resizing(image), tf.one_hot(label, num_classes)\\n\\n\\n# Shuffle the dataset to increase diversity of batches.\\n# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\\n# shuffle buffers.\\ntrain_dataset = train_dataset.shuffle(\\n    10 * BATCH_SIZE, reshuffle_each_iteration=True\\n).map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\\n\\nimages = next(iter(train_dataset.take(1)))[0]\\nkeras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\\n#setting the path to the directory containing the pics\\npath = \\'G:\\\\06  Projekte\\\\72 KI\\\\01 Studenten\\\\2023_06_Couture\\\\ball_tracking\\\\object_detected\\'\\n#appending the pics to the training data list\\ntraining_data = [] \\nborderType = cv2.BORDER_CONSTANT\\nvalue = [0, 0, 0]\\nsize=64\\n\\nfor img in os.listdir(path):\\n    pic = cv2.imread(os.path.join(path,img))\\n    \\n    more=(size- pic.shape[0])\\n    top = 0\\n    bottom = more\\n    more1=(size- pic.shape[1])\\n    left = 0\\n    right = more1\\n    \\n    a = cv2.copyMakeBorder(pic, top, bottom, left, right, borderType,value=value)\\n    \\n    \\n    pic = cv2.resize(pic,(size,size))\\n    training_data.append([pic])\\n    \\n#converting the list to numpy array and saving it to a file using #numpy.save\\nnp.save(os.path.join(\"./\",\\'aaa_false_set\\'),np.array(training_data))\\n#loading the saved file once again\\nsaved = np.load(os.path.join(\"./\",\\'aaa_data_set.npy\\'))\\nprint(saved.shape)\\nplt.imshow(saved[0].reshape(size,size,3))\\nplt.imshow(np.array(training_data[0]).reshape(size,size,3))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#classifier = keras_cv.models.ImageClassifier.from_preset(\n",
    "#    \"efficientnetv2_b0_imagenet_classifier\"\n",
    "#)\n",
    "\n",
    "image = keras.utils.load_img(\"Imageterrain.jpg\")\n",
    "image = np.array(image)\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n",
    ")\n",
    "predictions = classifier.predict(np.expand_dims(image, axis=0))\n",
    "top_classes = predictions[0].argsort(axis=-1)\n",
    "classes = keras.utils.get_file(\n",
    "    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\n",
    ")\n",
    "with open(classes, \"rb\") as f:\n",
    "    classes = json.load(f)\n",
    "top_two = [classes[str(i)] for i in top_classes[-2:]]\n",
    "print(\"Top two classes are:\", top_two)\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "data, dataset_info = tfds.load(\"cats_vs_dogs\", with_info=True, as_supervised=True)\n",
    "train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\n",
    "train_dataset = data[\"train\"]\n",
    "\n",
    "num_classes = dataset_info.features[\"label\"].num_classes\n",
    "\n",
    "resizing = keras_cv.layers.Resizing(\n",
    "    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_inputs(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Staticly resize images as we only iterate the dataset once.\n",
    "    return resizing(image), tf.one_hot(label, num_classes)\n",
    "\n",
    "\n",
    "# Shuffle the dataset to increase diversity of batches.\n",
    "# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\n",
    "# shuffle buffers.\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    10 * BATCH_SIZE, reshuffle_each_iteration=True\n",
    ").map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "images = next(iter(train_dataset.take(1)))[0]\n",
    "keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\n",
    "#setting the path to the directory containing the pics\n",
    "path = 'G:\\\\06  Projekte\\\\72 KI\\\\01 Studenten\\\\2023_06_Couture\\\\ball_tracking\\\\object_detected'\n",
    "#appending the pics to the training data list\n",
    "training_data = [] \n",
    "borderType = cv2.BORDER_CONSTANT\n",
    "value = [0, 0, 0]\n",
    "size=64\n",
    "\n",
    "for img in os.listdir(path):\n",
    "    pic = cv2.imread(os.path.join(path,img))\n",
    "    \"\"\"\"\"\"\n",
    "    more=(size- pic.shape[0])\n",
    "    top = 0\n",
    "    bottom = more\n",
    "    more1=(size- pic.shape[1])\n",
    "    left = 0\n",
    "    right = more1\n",
    "    \n",
    "    a = cv2.copyMakeBorder(pic, top, bottom, left, right, borderType,value=value)\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    pic = cv2.resize(pic,(size,size))\n",
    "    training_data.append([pic])\n",
    "    \n",
    "#converting the list to numpy array and saving it to a file using #numpy.save\n",
    "np.save(os.path.join(\"./\",'aaa_false_set'),np.array(training_data))\n",
    "#loading the saved file once again\n",
    "saved = np.load(os.path.join(\"./\",'aaa_data_set.npy'))\n",
    "print(saved.shape)\n",
    "plt.imshow(saved[0].reshape(size,size,3))\n",
    "plt.imshow(np.array(training_data[0]).reshape(size,size,3))\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7907527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "very blurry\n",
      "(32, 32, 3)\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.499999999999996\n",
      "29.499999999999996\n",
      "29.999999999999993\n",
      "28.999999999999996\n",
      "---------------------------------------------------------------------------------------------\n",
      "[[[29.5 61.  70. ]\n",
      "  [30.  61.  70. ]]\n",
      "\n",
      " [[29.5 61.5 69.5]\n",
      "  [29.  64.  70. ]]] \n",
      "------\n",
      " [[[  0.    0.  127.5]\n",
      "  [  0.    0.  127. ]]\n",
      "\n",
      " [[ 15.    6.  127.5]\n",
      "  [ 15.    6.  127.5]]]\n",
      "\n",
      "------\n",
      " 107.83333333333333\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "from ipynb.fs.full.subtraction import boundingBoxes,display,drawContour,substraction,substractionSAD\n",
    "from ipynb.fs.full.checkB import checkBoxes,checkBlob,initBlobDetect\n",
    "from ipynb.fs.full.wavelet import middle,Wavelet\n",
    "from ipynb.fs.full.ransac import RANSACcoude,plotLines,drawLines\n",
    "from ipynb.fs.full.white_select import selectWhite,selectWhiteHSV\n",
    "#from ipynb.fs.full.particle_track import particlesDetect,initialize_particles\n",
    "from ipynb.fs.full.particle_Wave import particlesDetect,initialize_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88bc3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lc100/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    }
   ],
   "source": [
    "def detect_ball(model, frame: np.ndarray):\n",
    "    WIDTH = 360\n",
    "    HEIGHT = 360\n",
    "    resized_frame = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "    detection = model(resized_frame)\n",
    "    bounding_box = detection.xyxy[0].numpy()\n",
    "\n",
    "    for box in bounding_box:\n",
    "        if box[5] == 0:\n",
    "            x = box[0] + (box[2] - box[0]) / 2\n",
    "            y = box[1] + (box[3] - box[1]) / 2\n",
    "            return (int(x), int(y)), (box[0], box[1], box[2], box[3])\n",
    "            # array([     124.15,         179,      130.78,      185.98,      0.8651,           0], dtype=float32)\n",
    "            # box[0]:   Left\n",
    "            # box[1]:   Top\n",
    "            # box[2]:   Right\n",
    "            # box[3]:   Bottom\n",
    "            # box[4]:   Probability\n",
    "            # box[5]:   Klasse 0 = Ball\n",
    "\n",
    "    return False, False\n",
    "\n",
    "ball_model = torch.hub.load('ultralytics/yolov5', 'custom', path='03_Ball_Detection/models/ball_weights_V2.pt', force_reload=False)\n",
    "\n",
    "def detect_ball_static(image,model=ball_model):\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # start object detection\n",
    "    # ball detection returns center coordinates from the ball\n",
    "    ball_center, ball_bb = detect_ball(model, image)\n",
    "    size=360\n",
    "    if ball_center:  \n",
    "        ball_center = [ball_center[0] * (width / size), ball_center[1] * (height / size)] \n",
    "        ball_bb=[int(ball_bb[0]* (width / size)),\n",
    "                int(ball_bb[1]* (height / size)),\n",
    "                int(ball_bb[2]* (width / size)),\n",
    "                int(ball_bb[3]* (height / size))]  \n",
    "        \n",
    "        return [ball_bb[0],ball_bb[1],ball_bb[2]-ball_bb[0]+20,ball_bb[3]-ball_bb[1]+20]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78182fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 1080\n"
     ]
    }
   ],
   "source": [
    "# 2 frames for the substraction\n",
    "frame1=None #newest frame\n",
    "frame0=None #previous frame\n",
    "\n",
    "background=None\n",
    "\n",
    "# 1st frame will be use to detect the white borders of the field\n",
    "first_frame=None\n",
    "white_border=None\n",
    "\n",
    "# time jump between 2 frame\n",
    "jump=100 #60\n",
    "# index is the time of the frame\n",
    "idx=jump*2\n",
    "\n",
    "#3D points (x,y,t,type)\n",
    "wavePoint=[[0,0,0,0]]\n",
    "#3D Blob points (x,y,t,type)\n",
    "blobPoint=[[0,0,0,0]]\n",
    "#3D points particle(x,y,t)\n",
    "particlePoint=[[0,0,0]]\n",
    "\n",
    "#3D points wavePoint+particlePoint (x,y,t,type), 0=move, 1=static, 2=particle\n",
    "allPoints=[[0,0,0,0]]\n",
    "\n",
    "\n",
    "#number of pts used for RANSAC\n",
    "size_slice=30 #40  #20\n",
    "#dist min for RANSAC\n",
    "distMinRANSAC=30 #35 #30     #25 #20 #70\n",
    "#min pts for Ransac being correct\n",
    "minPtsRansac=float(size_slice)*1/3\n",
    "#min pts on each branche\n",
    "Wbranche=5\n",
    "#pts after ransac\n",
    "corrected=np.array([[0,0,0,0]])\n",
    "#pts after correction of the trajectory\n",
    "corrected2=np.array([[0,0,0,0]])\n",
    "# index of the last pts that had is trajectory corrected in corrected[]\n",
    "lastcorrected=0 \n",
    "firstLoop=True\n",
    "lastLen=0\n",
    "\n",
    "# all the lines found by RANSAC\n",
    "all_lines=[ [ [[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]] ] ]\n",
    "#all_lines=[[[0,0,0],[0,0,0]]]\n",
    "\n",
    "model=None\n",
    "lastwavePoint=None\n",
    "\n",
    "static=False\n",
    "static_box=None\n",
    "\n",
    "\n",
    "#templates used for the wavelet function templateOutput\n",
    "#template = cv2.imread('template/templateOutput.png')\n",
    "template = cv2.imread('template/templateRGB_both.png')\n",
    "template_W= Wavelet(template)\n",
    "\n",
    "templateB = cv2.imread('template/templateB.png')\n",
    "template_B = Wavelet(templateB)\n",
    "\n",
    "#video\n",
    "cap = cv2.VideoCapture('video_record/1310.mp4')\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "    \n",
    "\n",
    "# Capture frame-by-frame\n",
    "ret, frame_og= cap.read()\n",
    "\n",
    "\n",
    "#particle detect\n",
    "W=1700#np.shape(frame_og)[1]\n",
    "H=np.shape(frame_og)[0]\n",
    "print(W,H)\n",
    "N=50 #wave=50\n",
    "VEL=40.0\n",
    "POS_SIGMA = 60.0\n",
    "particles = initialize_particles(N=N,width=W,height=H,velocity=VEL)\n",
    "\n",
    "detector=initBlobDetect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56233ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob green: 1676.0\n",
      "blob green: 1646.0\n",
      "blob green: 1713.0\n",
      "blob green: 1936.0\n",
      "blob green: 3080.0\n",
      "blob green: 2116.0\n",
      "blob green: 1936.0\n",
      "blob green: 2039.0\n",
      "blob green: 1936.0\n",
      "blob green: 2603.0\n",
      "blob green: 3495.0\n",
      "blob green: 3111.0\n",
      "blob green: 2807.0\n",
      "blob green: 2804.0\n",
      "blob green: 2839.0\n",
      "blob green: 2738.0\n",
      "blob green: 2597.0\n",
      "blob green: 2507.0\n",
      "blob green: 2145.0\n",
      "blob green: 2006.0\n",
      "blob green: 1952.0\n",
      "blob green: 1548.0\n",
      "blob green: 1787.0\n",
      "blob green: 1683.0\n",
      "blob green: 1977.0\n",
      "blob green: 1556.0\n",
      "blob green: 1647.0\n",
      "blob green: 1601.0\n",
      "blob green: 2537.0\n",
      "blob green: 1960.0\n",
      "blob green: 1939.0\n",
      "blob green: 2151.0\n",
      "blob green: 1539.0\n",
      "blob green: 1673.0\n",
      "blob green: 2125.0\n",
      "blob green: 2633.0\n",
      "blob green: 3102.0\n",
      "blob green: 1917.0\n",
      "blob green: 3460.0\n",
      "blob green: 1564.0\n",
      "blob green: 3130.0\n",
      "blob green: 2513.0\n",
      "blob green: 2573.0\n",
      "blob green: 2656.0\n",
      "blob green: 1736.0\n",
      "blob green: 2796.0\n",
      "blob green: 2017.0\n",
      "blob green: 2031.0\n",
      "blob green: 2040.0\n",
      "blob green: 2125.0\n",
      "blob green: 2208.0\n",
      "blob green: 2004.0\n",
      "blob green: 1983.0\n",
      "blob green: 1774.0\n",
      "blob green: 2747.0\n",
      "blob green: 1525.0\n",
      "blob green: 1605.0\n",
      "blob green: 1635.0\n",
      "blob green: 1528.0\n",
      "blob green: 1803.0\n",
      "blob green: 2017.0\n",
      "blob green: 1666.0\n",
      "blob green: 1887.0\n",
      "blob green: 2352.0\n",
      "blob green: 1665.0\n",
      "blob green: 2221.0\n",
      "blob green: 2657.0\n",
      "blob green: 1815.0\n",
      "blob green: 1702.0\n",
      "blob green: 2916.0\n",
      "blob green: 1528.0\n",
      "blob green: 4096.0\n",
      "blob green: 2793.0\n",
      "blob green: 2742.0\n",
      "blob green: 3758.0\n",
      "blob green: 3004.0\n",
      "blob green: 2165.0\n",
      "blob green: 3116.0\n",
      "blob green: 2496.0\n",
      "blob green: 1899.0\n",
      "blob green: 2154.0\n",
      "blob green: 1614.0\n",
      "blob green: 1947.0\n",
      "blob green: 2074.0\n",
      "blob green: 1936.0\n",
      "blob green: 2037.0\n",
      "blob green: 1570.0\n",
      "blob green: 1668.0\n",
      "blob green: 1504.0\n",
      "blob green: 1592.0\n",
      "blob green: 1888.0\n",
      "blob green: 1562.0\n",
      "blob green: 1803.0\n",
      "blob green: 2116.0\n",
      "blob green: 1504.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m (box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m static) : \u001b[39m#did not detected moving ball, static mode on\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     static\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m    \n\u001b[1;32m--> 124\u001b[0m     box\u001b[39m=\u001b[39mdetect_ball_static(frame_og_crop\u001b[39m.\u001b[39;49mcopy(),ball_model)     \n\u001b[0;32m    125\u001b[0m     static_box\u001b[39m=\u001b[39m box\n\u001b[0;32m    126\u001b[0m \u001b[39melif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m static_box \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# still not moving, use the last detection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mdetect_ball_static\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m     27\u001b[0m height, width, _ \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape\n\u001b[0;32m     29\u001b[0m \u001b[39m# start object detection\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# ball detection returns center coordinates from the ball\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m ball_center, ball_bb \u001b[39m=\u001b[39m detect_ball(model, image)\n\u001b[0;32m     32\u001b[0m size\u001b[39m=\u001b[39m\u001b[39m360\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m ball_center:  \n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mdetect_ball\u001b[1;34m(model, frame)\u001b[0m\n\u001b[0;32m      3\u001b[0m HEIGHT \u001b[39m=\u001b[39m \u001b[39m360\u001b[39m\n\u001b[0;32m      4\u001b[0m resized_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(frame, (WIDTH, HEIGHT))\n\u001b[1;32m----> 5\u001b[0m detection \u001b[39m=\u001b[39m model(resized_frame)\n\u001b[0;32m      6\u001b[0m bounding_box \u001b[39m=\u001b[39m detection\u001b[39m.\u001b[39mxyxy[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m bounding_box:\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:705\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[0;32m    703\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 705\u001b[0m         y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, augment\u001b[39m=\u001b[39;49maugment)  \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[39m# Post-process\u001b[39;00m\n\u001b[0;32m    708\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:515\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    512\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[0;32m    516\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\yolo.py:209\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[0;32m    208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\yolo.py:121\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 121\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m    122\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:167\u001b[0m, in \u001b[0;36mC3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv3(torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv2(x)), \u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:59\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\lc100\\AppData\\Local\\miniconda3\\envs\\frameSubtraction\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame_og= cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        model=None\n",
    "        \n",
    "        frame_og_crop=frame_og[:,:1700,:].copy()\n",
    "        last_frame = frame_og_crop.copy()\n",
    "\n",
    "        #detect the white border so to not confuse them with the ball ######################################################################################################################################\n",
    "        if first_frame is None:\n",
    "            frame0 = frame_og_crop.copy()\n",
    "            frame1 = frame_og_crop.copy()\n",
    "            \n",
    "            background = frame_og_crop.copy()\n",
    "            background = cv2.medianBlur(background, 21)\n",
    "            background = cv2.cvtColor(background, cv2.COLOR_BGR2YCR_CB)\n",
    "            #background = background[:,:,0]\n",
    "            background = background.astype('float32')\n",
    "            \n",
    "            first_frame = frame_og_crop.copy()\n",
    "            white_border=selectWhite(first_frame,dilate=9,erode=2)\n",
    "\n",
    "            white_border=cv2.bitwise_not(white_border)\n",
    "\n",
    "            #display(white_border,name='not border')\n",
    "\n",
    "    \n",
    "        frame0 = frame1.copy()\n",
    "        frame1 = frame_og_crop.copy()\n",
    "\n",
    "        #white select\n",
    "        #maskW1=selectWhite(frame1,erode=3,dilate=8,B_limit=70)\n",
    "        #maskW0=selectWhite(frame0,erode=3,dilate=8,B_limit=70)\n",
    "\n",
    "        #maskW=cv2.bitwise_or(maskW1, maskW0, mask = None)\n",
    "        #maskW=cv2.bitwise_and(maskW,white_border, mask=None)\n",
    "        #maskW = cv2.erode(maskW, None, iterations=4)\n",
    "        #maskWA = cv2.dilate(maskW, None, iterations=15)\n",
    "        \n",
    "        #frame_white=cv2.bitwise_and(frame_og_crop.copy(),frame_og_crop.copy(), mask=maskW)\n",
    "        #display(frame_white,name='white mask RGB')\n",
    "        \n",
    "        \n",
    "        #substraction ######################################################################################################################################\n",
    "        frameHLS0=cv2.cvtColor(frame0,cv2.COLOR_BGR2YUV)\n",
    "        frameHLS1=cv2.cvtColor(frame1,cv2.COLOR_BGR2YUV)\n",
    "        \n",
    "        frame_substraction=substraction(frameHLS0[:,:,0],frameHLS1[:,:,0],blur_type=1,blur=21,threshold_type=0,threshold=15,show=False,erode=4,dilate=10)\n",
    "        \n",
    "        dil,contours=drawContour(frame_substraction,kernelsize=25)\n",
    "        #display(dil,name='dilatation')\n",
    "        #put boxes around objects\n",
    "        maskRec,boundRect=boundingBoxes(contours,show=False,width=W,height=H)\n",
    "        #detect the ball in one of the boxes (if any)\n",
    "        box = checkBoxes(frame_og_crop.copy(),boundRect,coeff=15.0,wave_temp=template_W,wave_temp2=template_B,show=False) #coeff=15\n",
    "        \n",
    "        \n",
    "        #Subtraction background and blob ######################################################################################################################################\n",
    "        \n",
    "        background,frame_subBackground =substractionSAD(background,frame0.copy(),frame1.copy(),idx//jump,show=True,blur=15,threshold=10.0)\n",
    "        #frame_subBackground = cv2.bitwise_and(frame_substraction , white_border, mask=None)\n",
    "        \n",
    "        ptsBlob=checkBlob(detector,frame_subBackground,frame1.copy(),idx,show=True)\n",
    "        \n",
    "        if ptsBlob is not None:\n",
    "            allPoints=np.concatenate((allPoints,ptsBlob))\n",
    "            blobPoint=np.concatenate((blobPoint,ptsBlob))\n",
    "          \n",
    "            \n",
    "        #white select ######################################################################################################################################\n",
    "        \"\"\"\n",
    "        maskW_sub=cv2.bitwise_and(frame_og_crop.copy(), frame_og_crop.copy(), mask=dil)\n",
    "        maskW_sub=selectWhite(maskW_sub,erode=3,dilate=8,B_limit=60,W_limit=100,H_limit=20)\n",
    "\n",
    "        maskW = cv2.bitwise_and(maskW_sub , white_border, mask=None)\n",
    "        maskW = cv2.erode(maskW, None, iterations=4)\n",
    "        maskW = cv2.dilate(maskW, None, iterations=6)\n",
    "        \n",
    "        whiteANDsub=cv2.bitwise_and(frame_og_crop.copy(),frame_og_crop.copy(), mask=maskW)\n",
    "        display(whiteANDsub,name='select white')\n",
    "        \"\"\"\n",
    "        \n",
    "        #particle ######################################################################################################################################\n",
    "        \n",
    "        #static boxes\n",
    "        #if static_box is not None:\n",
    "        #   cv2.rectangle(maskRec, (int(static_box[0]), int(static_box[1])),(int(static_box[0]+static_box[2]), int(static_box[1]+static_box[3])),1, -1)\n",
    "        \n",
    "        frameRGB = frame_og_crop.copy()\n",
    "        frameRGB=cv2.bitwise_and(frameRGB, frameRGB, mask=maskRec)\n",
    "        particles,terminate,location=particlesDetect(particles,frameRGB,frame_og_crop.copy(),N=N,width=W,height=H,sigma=POS_SIGMA)\n",
    "        location=None\n",
    "        \n",
    "        \"\"\"\n",
    "        #Color\n",
    "        frameHLS= cv2.cvtColor(frameRGB,cv2.COLOR_BGR2HLS)\n",
    "        frameHLS=cv2.bitwise_and(frameHLS, frameHLS, mask=white_border)\n",
    "        \n",
    "        TARGET_COLOUR_WHITE_HLS = np.array((200,255,40))\n",
    "        DIFF_COLOR_HLS=(160,70,40) # lower color= target-diff\n",
    "        \n",
    "        #TARGET_COLOUR_WHITE = np.array((255,255,255))\n",
    "        #DIFF_COLOR=(120,120,120) # lower color= target-diffq\n",
    "        \n",
    "        particles,terminate,location=particlesDetect(particles,frameHLS,N=N,width=W,height=H,sigma=POS_SIGMA,colour=TARGET_COLOUR_WHITE_HLS,diff_color=DIFF_COLOR_HLS)\n",
    "        \"\"\"\n",
    "        \n",
    "        if location is not None:\n",
    "            P=[location[1],location[0],idx]\n",
    "            particlePoint=np.append(particlePoint,[P],axis=0) \n",
    "            \n",
    "            b=[location[1],location[0],idx,0.0]\n",
    "            allPoints=np.append(allPoints,[b],axis=0)  \n",
    "            \n",
    "        \n",
    "        \n",
    "        #static detection ######################################################################################################################################\n",
    "        if (box is None and not static) : #did not detected moving ball, static mode on\n",
    "            static=True    \n",
    "            box=detect_ball_static(frame_og_crop.copy(),ball_model)     \n",
    "            static_box= box\n",
    "        elif box is None and static_box is not None: # still not moving, use the last detection\n",
    "            box=static_box\n",
    "        else: # moving, static mode off\n",
    "            static_box=None\n",
    "            static=False   \n",
    "        \n",
    "        # add the 3D to the points\n",
    "        if box is not None:\n",
    "            pts=middle(box)\n",
    "            \n",
    "            # if same wavePoint, don't take it.\n",
    "            if not np.array_equal(pts, wavePoint[-1][:2]) and ((pts[0]-wavePoint[-1][0])**2+(pts[1]-wavePoint[-1][1])**2)>70:\n",
    "                if static:\n",
    "                    P=[pts[0],pts[1],idx,1.5]\n",
    "                else:\n",
    "                    P=[pts[0],pts[1],idx,0.75]\n",
    "                wavePoint=np.append(wavePoint,[P],axis=0) \n",
    "                allPoints=np.append(allPoints,[P],axis=0) \n",
    "\n",
    "            cv2.rectangle(frame_og_crop, (int(box[0]), int(box[1])),(int(box[0]+box[2]), int(box[1]+box[3])), (255,255,255), 2) \n",
    "            \n",
    "\n",
    "\n",
    "        #Ransac ######################################################################################################################################\n",
    "        \n",
    "        #tab=wavePoint\n",
    "        tab=allPoints\n",
    "        \n",
    "        Len=len(allPoints)\n",
    "        #Len1=len(wavePoint)\n",
    "        \n",
    "        if Len%size_slice==0 and lastLen!=Len:\n",
    "            \n",
    "            if firstLoop:\n",
    "                tab=tab[1:]\n",
    "\n",
    "            sliced=tab[(Len-size_slice):]\n",
    "            \n",
    "            \n",
    "            if model is not None:\n",
    "                sliced=np.concatenate((corrected[len(corrected)-5:],sliced))\n",
    "            elif firstLoop==False:\n",
    "                sliced=np.concatenate((tab[(Len-5-size_slice):(Len-1-size_slice)],sliced))\n",
    "            \n",
    "            sliced=np.concatenate((tab[(Len-10-size_slice):(Len-1-size_slice)],sliced))\n",
    "            \n",
    "            model,droite=RANSACcoude(sliced,N=sliced.shape[0]**2,distanceMin=distMinRANSAC,wheightBranche=Wbranche,minPts=minPtsRansac)\n",
    "            lastLen=Len\n",
    "\n",
    "            if model is not None:\n",
    "                corrected=np.concatenate((corrected,model))\n",
    "                all_lines=np.append(all_lines,[droite],axis=0)\n",
    "                \n",
    "\n",
    "                if firstLoop:\n",
    "                    corrected=corrected[1:]\n",
    "                    all_lines=all_lines[1:]\n",
    "                    \n",
    "      \n",
    "        # correct trajectory: if i is shorter to go to the jumpCth pts rather than the next then we jump ####################################################\n",
    "        \n",
    "        k=lastcorrected\n",
    "        lenCorrect=len(corrected)\n",
    "        jumpC=3\n",
    "        \"\"\"\n",
    "        while k < (lenCorrect-5) :\n",
    "            diff1=cv2.absdiff(corrected[k],corrected[k+1])\n",
    "            diff2=cv2.absdiff(corrected[k],corrected[k+jumpC])\n",
    "            corrected2=np.append(corrected2,[corrected[k]],axis=0)\n",
    "\n",
    "            if diff1[0]+diff1[1]*1 < (diff2[0]+diff2[1]):\n",
    "                k+=1\n",
    "            else:\n",
    "                k+=jumpC-1\n",
    "        \"\"\"\n",
    "        if firstLoop and model is not None:\n",
    "            corrected2=corrected2[1:]\n",
    "            firstLoop=False\n",
    "        if lenCorrect>5:\n",
    "            lastcorrected = lenCorrect-jumpC\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #plot######################################################################################################################################\n",
    "          \n",
    "        for index, item in enumerate(particlePoint): \n",
    "            cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [20, 225, 255], 5)\n",
    "        for index, item in enumerate(wavePoint): \n",
    "            if item[3]==0.75: #green is moving\n",
    "                cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [20, 255, 20], 5)\n",
    "            else: # blue is static\n",
    "                cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [255, 200, 50], 5)\n",
    "        for index, item in enumerate(blobPoint): \n",
    "            cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [200, 80, 255], 5)\n",
    "            \n",
    "        \"\"\"\n",
    "        for index, item in enumerate(allPoints): \n",
    "            cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [20, 255, 20], 5)\n",
    "        \"\"\"    \n",
    "             \n",
    "        #plotLines(particlePointointoint,frame=frame_og_crop,disappear=True,limit=20) \n",
    "        #plotLines(wavePoint,frame=frame_og_crop,disappear=False)\n",
    "        #plotLines(corrected,frame=frame_og_crop,disappear=False) \n",
    "        drawLines(all_lines,frame_og_crop,thick=distMinRANSAC)\n",
    "        #plotLines(corrected2,frame=frame_og_crop,disappear=False,color= [255, 10, 10]) \n",
    "        \n",
    "        for index, item in enumerate(corrected): \n",
    "            cv2.circle(frame_og_crop, (int(item[0]),int(item[1])), 2, [20, 20, 255], 5)\n",
    "\n",
    "        display(frame_og_crop,name='detection')\n",
    "        \n",
    "        idx+=jump\n",
    "        \n",
    "        #if idx==58600:\n",
    "        \n",
    "            \n",
    "        #next loop ######################################################################################################################################\n",
    "        \"\"\"\n",
    "        #press key to go the next frame or Q to exit or S to save the frame\n",
    "        key = cv2.waitKey(0)\n",
    "        \n",
    "        if key == ord('q') or key == ord('Q'):\n",
    "            break\n",
    "        elif key == ord('s') or key == ord('S'):\n",
    "            #box_cp = frame_og[box[1]:(box[1]+box[3]),box[0]:(box[0]+box[2]),:]\n",
    "            #cv2.imwrite(\"object1_{}.png\".format(idx), box_cp)\n",
    "            cv2.imwrite(\"image.png\", frame_og_crop)\n",
    "        else:\n",
    "            continue\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        if cv2.waitKey(1)==27:\n",
    "            if not cv2.waitKey(1)==27:\n",
    "                break\n",
    "            \n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0265e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index, item in enumerate(wavePoint): \\n    if not item[3]: #green is moving\\n        cv2.circle(frame_og_crop, item[:2], 2, [20, 255, 20], 5)\\n    else: # blue is static\\n        cv2.circle(frame_og_crop, item[:2], 2, [255, 200, 50], 5)\\nplotLines(corrected,frame=frame_og_crop,disappear=False) # draw boxe selected\\ncv2.imwrite(\"image.png\", frame_og_crop)\\n\\nprint(wavePoint.shape)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Closes all the frames\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\"\n",
    "for index, item in enumerate(wavePoint): \n",
    "    if not item[3]: #green is moving\n",
    "        cv2.circle(frame_og_crop, item[:2], 2, [20, 255, 20], 5)\n",
    "    else: # blue is static\n",
    "        cv2.circle(frame_og_crop, item[:2], 2, [255, 200, 50], 5)\n",
    "plotLines(corrected,frame=frame_og_crop,disappear=False) # draw boxe selected\n",
    "cv2.imwrite(\"image.png\", frame_og_crop)\n",
    "\n",
    "print(wavePoint.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c90e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8)) ######################################################################################################################################\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.grid()\n",
    "\n",
    "#ax.scatter(allPoints[:,1],allPoints[:,0],allPoints[:,2],marker=\"+\",color='g')\n",
    "#ax.scatter(corrected[:,1],corrected[:,0],corrected[:,2],marker=\"o\",color='r')\n",
    "\n",
    "ax.scatter(wavePoint[:,1],wavePoint[:,0],wavePoint[:,2],marker=\"o\",color='g')\n",
    "#ax.scatter(particlePoint[:,1],particlePoint[:,0],particlePoint[:,2],marker=\"+\")\n",
    "ax.scatter(blobPoint[:,1],blobPoint[:,0],blobPoint[:,2],marker=\",\")\n",
    "\n",
    "#for index, item in enumerate(wavePoint): \n",
    "#     if (item[2]//jump)%15==0:\n",
    "#          ax.scatter(item[1],item[0],item[2],marker=\"o\",color='r')\n",
    "\n",
    "\n",
    "ax.set_ylabel('x', labelpad=20)\n",
    "ax.set_ylim(0, 1700)\n",
    "ax.set_xlabel('y', labelpad=20)\n",
    "ax.set_xlim(0, 1100)\n",
    "ax.set_zlabel('t', labelpad=20)\n",
    "#ax.set_zlim(0, 100)\n",
    "\n",
    "fig2 = plt.figure(figsize = (8,8)) ######################################################################################################################################\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "ax2.grid()\n",
    "ax2.scatter(corrected[:,1],corrected[:,0],corrected[:,2],marker='+',color='g')\n",
    "\n",
    "for item in all_lines:\n",
    "     start_pts=item[0][0]\n",
    "     middle_pts=item[0][1]\n",
    "     end_pts=item[1][1]\n",
    "     \n",
    "     ax2.scatter(middle_pts[1],middle_pts[0],middle_pts[2],color='g')\n",
    "     ax2.scatter(end_pts[1],end_pts[0],end_pts[2],color='b')\n",
    "     ax2.scatter(start_pts[1],start_pts[0],start_pts[2],color='r')\n",
    "     \n",
    "     ax2.plot([start_pts[1], middle_pts[1]], [start_pts[0], middle_pts[0]], zs=[start_pts[2], middle_pts[2]],color='k')\n",
    "     ax2.plot([end_pts[1], middle_pts[1]], [end_pts[0], middle_pts[0]], zs=[end_pts[2], middle_pts[2]],color='k')\n",
    "\n",
    "ax2.set_ylabel('x', labelpad=20)\n",
    "ax2.set_ylim(0, 1700)\n",
    "ax2.set_xlabel('y', labelpad=20)\n",
    "ax2.set_xlim(0, 1100)\n",
    "ax2.set_zlabel('t', labelpad=20)\n",
    "\n",
    "\"\"\"\n",
    "fig = plt.figure(figsize = (8,8)) ######################################################################################################################################\n",
    "\n",
    "plt.plot(corrected[:,2],corrected[:,1],'g')\n",
    "plt.plot(corrected[:,2],corrected[:,0],'r')\n",
    "plt.plot(corrected[:,2],corrected[:,1],'g+')\n",
    "plt.plot(corrected[:,2],corrected[:,0],'r+')\n",
    "\n",
    "fig = plt.figure(figsize = (8,8)) ######################################################################################################################################\n",
    "\n",
    "plt.plot(corrected2[:,2],corrected2[:,1],'k')\n",
    "plt.plot(corrected2[:,2],corrected2[:,0],'b')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "key = cv2.waitKey(0)\n",
    "#When everything done, release the video capture object\n",
    "cap.release()  \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ff9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#classifier = keras_cv.models.ImageClassifier.from_preset(\\n#    \"efficientnetv2_b0_imagenet_classifier\"\\n#)\\n\\nimage = keras.utils.load_img(\"Imageterrain.jpg\")\\nimage = np.array(image)\\nkeras_cv.visualization.plot_image_gallery(\\n    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\\n)\\npredictions = classifier.predict(np.expand_dims(image, axis=0))\\ntop_classes = predictions[0].argsort(axis=-1)\\nclasses = keras.utils.get_file(\\n    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\\n)\\nwith open(classes, \"rb\") as f:\\n    classes = json.load(f)\\ntop_two = [classes[str(i)] for i in top_classes[-2:]]\\nprint(\"Top two classes are:\", top_two)\\nBATCH_SIZE = 32\\nIMAGE_SIZE = (224, 224)\\nAUTOTUNE = tf.data.AUTOTUNE\\ntfds.disable_progress_bar()\\n\\ndata, dataset_info = tfds.load(\"cats_vs_dogs\", with_info=True, as_supervised=True)\\ntrain_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\\ntrain_dataset = data[\"train\"]\\n\\nnum_classes = dataset_info.features[\"label\"].num_classes\\n\\nresizing = keras_cv.layers.Resizing(\\n    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\\n)\\n\\n\\ndef preprocess_inputs(image, label):\\n    image = tf.cast(image, tf.float32)\\n    # Staticly resize images as we only iterate the dataset once.\\n    return resizing(image), tf.one_hot(label, num_classes)\\n\\n\\n# Shuffle the dataset to increase diversity of batches.\\n# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\\n# shuffle buffers.\\ntrain_dataset = train_dataset.shuffle(\\n    10 * BATCH_SIZE, reshuffle_each_iteration=True\\n).map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\\n\\nimages = next(iter(train_dataset.take(1)))[0]\\nkeras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\\n#setting the path to the directory containing the pics\\npath = \\'G:\\\\06  Projekte\\\\72 KI\\\\01 Studenten\\\\2023_06_Couture\\\\ball_tracking\\\\object_detected\\'\\n#appending the pics to the training data list\\ntraining_data = [] \\nborderType = cv2.BORDER_CONSTANT\\nvalue = [0, 0, 0]\\nsize=64\\n\\nfor img in os.listdir(path):\\n    pic = cv2.imread(os.path.join(path,img))\\n    \\n    more=(size- pic.shape[0])\\n    top = 0\\n    bottom = more\\n    more1=(size- pic.shape[1])\\n    left = 0\\n    right = more1\\n    \\n    a = cv2.copyMakeBorder(pic, top, bottom, left, right, borderType,value=value)\\n    \\n    \\n    pic = cv2.resize(pic,(size,size))\\n    training_data.append([pic])\\n    \\n#converting the list to numpy array and saving it to a file using #numpy.save\\nnp.save(os.path.join(\"./\",\\'aaa_false_set\\'),np.array(training_data))\\n#loading the saved file once again\\nsaved = np.load(os.path.join(\"./\",\\'aaa_data_set.npy\\'))\\nprint(saved.shape)\\nplt.imshow(saved[0].reshape(size,size,3))\\nplt.imshow(np.array(training_data[0]).reshape(size,size,3))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#classifier = keras_cv.models.ImageClassifier.from_preset(\n",
    "#    \"efficientnetv2_b0_imagenet_classifier\"\n",
    "#)\n",
    "\n",
    "image = keras.utils.load_img(\"Imageterrain.jpg\")\n",
    "image = np.array(image)\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n",
    ")\n",
    "predictions = classifier.predict(np.expand_dims(image, axis=0))\n",
    "top_classes = predictions[0].argsort(axis=-1)\n",
    "classes = keras.utils.get_file(\n",
    "    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\n",
    ")\n",
    "with open(classes, \"rb\") as f:\n",
    "    classes = json.load(f)\n",
    "top_two = [classes[str(i)] for i in top_classes[-2:]]\n",
    "print(\"Top two classes are:\", top_two)\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "data, dataset_info = tfds.load(\"cats_vs_dogs\", with_info=True, as_supervised=True)\n",
    "train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\n",
    "train_dataset = data[\"train\"]\n",
    "\n",
    "num_classes = dataset_info.features[\"label\"].num_classes\n",
    "\n",
    "resizing = keras_cv.layers.Resizing(\n",
    "    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_inputs(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Staticly resize images as we only iterate the dataset once.\n",
    "    return resizing(image), tf.one_hot(label, num_classes)\n",
    "\n",
    "\n",
    "# Shuffle the dataset to increase diversity of batches.\n",
    "# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\n",
    "# shuffle buffers.\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    10 * BATCH_SIZE, reshuffle_each_iteration=True\n",
    ").map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "images = next(iter(train_dataset.take(1)))[0]\n",
    "keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\n",
    "#setting the path to the directory containing the pics\n",
    "path = 'G:\\\\06  Projekte\\\\72 KI\\\\01 Studenten\\\\2023_06_Couture\\\\ball_tracking\\\\object_detected'\n",
    "#appending the pics to the training data list\n",
    "training_data = [] \n",
    "borderType = cv2.BORDER_CONSTANT\n",
    "value = [0, 0, 0]\n",
    "size=64\n",
    "\n",
    "for img in os.listdir(path):\n",
    "    pic = cv2.imread(os.path.join(path,img))\n",
    "    \"\"\"\"\"\"\n",
    "    more=(size- pic.shape[0])\n",
    "    top = 0\n",
    "    bottom = more\n",
    "    more1=(size- pic.shape[1])\n",
    "    left = 0\n",
    "    right = more1\n",
    "    \n",
    "    a = cv2.copyMakeBorder(pic, top, bottom, left, right, borderType,value=value)\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    pic = cv2.resize(pic,(size,size))\n",
    "    training_data.append([pic])\n",
    "    \n",
    "#converting the list to numpy array and saving it to a file using #numpy.save\n",
    "np.save(os.path.join(\"./\",'aaa_false_set'),np.array(training_data))\n",
    "#loading the saved file once again\n",
    "saved = np.load(os.path.join(\"./\",'aaa_data_set.npy'))\n",
    "print(saved.shape)\n",
    "plt.imshow(saved[0].reshape(size,size,3))\n",
    "plt.imshow(np.array(training_data[0]).reshape(size,size,3))\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
